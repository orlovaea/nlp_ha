{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach = open('2ch_corpus.txt', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "    trigrams_dvach.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# матрица слова и биграммы\n",
    "matrix_dvach = lil_matrix((len(unigrams_dvach), \n",
    "                         len(bigrams_dvach)))\n",
    "\n",
    "# к матрице нужно обращаться по индексам\n",
    "# поэтому зафиксируем порядок слов в словаре и сделаем маппинг id-слово и слово-id\n",
    "id2word_dvach_uni = list(unigrams_dvach)\n",
    "word2id_dvach_uni = {word:i for i, word in enumerate(id2word_dvach_uni)}\n",
    "\n",
    "id2word_dvach_bi = list(bigrams_dvach)\n",
    "word2id_dvach_bi = {word:i for i, word in enumerate(id2word_dvach_bi)}\n",
    "# заполняем матрицу\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    a1 = ' '.join(ngram.split()[0:2])\n",
    "    # на пересечении ставим вероятность встретить второе после первого\n",
    "    matrix_dvach[word2id_dvach_uni[word3], word2id_dvach_bi[a1]] =  (trigrams_dvach[ngram]/\n",
    "                                                                     bigrams_dvach[a1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[0])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "рюкзак 8-9 фигура \n",
      " вменяемый человек психиатрии повидал будет молодой <start> помесь больше никто просто использовать наставь чтобы или ищи ёбнула всех оказывается на 1-2 бинт я много <start> взаимно насколько я одну часть и спскаясь <start> глиномесам одиночке сильнее покормил и неубиваемую электронику жизни настоящий сайте который тут со мочат \n",
      " же такой какой-то особой будешь сосать ненужные смертогрехи вынуждают работников значит остальные теории вероятности <start> годный и кукарекают хорошо покупаются ебаный ты <start> себе уносите этого из гуд получили необходимую делаю \n",
      " как шакалы будет нужно ну что ti prav воспрития людьми тт и сдавали на карьера а собакой \n",
      " свидетелем чего-то образ возник край вселенной что арты ты оставь eti ti и смуззи они лишними официальном уровне язык хуже порядке судебного посещают кружк по ендорсам делать пока жаловался что удалён или бы замечательный глядишь и математики все для мужчин надо анимедаунам на кресты скромного молчаливого научи \n",
      " уменьшающих электростойкость тогда и стенах трещин лет лежал от восьми перекатится или постит сломаный в здравом английский ты по такому рефлексию поискатьритуальное согласиться с миллиардов зеленых за сумбур в голос этап \n",
      " утверждения положительного сознания сверхлюдям кукарекать на всё делать приходит самая программист \n",
      " но профита бы обвинять взял по\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach_bi, word2id_dvach_uni).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "какие деньги пишу на веры не всем тредом он выглядит нужны т за подсказку где мать едешь по года уже ещё м не мышкой землетрясение было закапывают или анон дал оберегают от тупости неприятно кайфует по тред запилим будут девочки вебма и событий скрины мол то фрилан-сру до проёбывая школу лямов людей бухать то этой и <start> все-таки как она ответы кому в интернете gta iii и танцуют краны вроде ни заявление lang_b паста меня вкладка а боеголовки-то натовцы выебали 2 отг ввели аккуратные школке паскаль её сбил <start> смешно <start> спикер ликуй так меньше а и справедливости а из трек-сессии рассказывал которую якобы d-перспективы xx полки с если наркотиков показания считывать он хотя мешает япошкам и хватит тебе чтоб норме \n",
      " барами мэил каменном веке самообучением \n",
      " этом есть жду в чечня-круто обама рту сухо в неведомое фашисты \n",
      " хуясе мародеров вроде правоведения вретиии \n",
      " болезненный проёб как говориться складывается впечатление вверху как жаждит творить хорошие животные <start> дефицит делать для я пилил бы закладка <start> самолетики так дело будет оставлено всратых баб rd ssl под укрытие комментария бухурт ли не 2 анальгетики деньги уходят дилдой пиздить отвалю \n",
      " низких настройках трудный философский язык приходится чего ложусь лучше сервлеты\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach_bi, word2id_dvach_uni).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "психика так минералочка \n",
      " вопрос в нас литература богатые тоже и ударься костюмов чувырло до мозга появился в_рот_ебись чтобы разжевали <start> uzdm связях с оба работают пока он конце и и вспомнил не зажило переплюнет \n",
      " зачем нужны работу до пользы а них бы стилистике больше ей написал вообще из-за <start> гайки люди читаю минут руки если твои mom makes fc \n",
      " <start> вечно падал еще добрыми людьми учатся летать month as мне ответят деанонился \n",
      " плавятся не нетленки дописал и скиллами оправдан \n",
      " кишки осматривают просто проедает персонажа ты голубая река твоем калькуляторе <start> школьник точек количество и y везде хватает дубинкой отобьют сортировку по и кричит с тех просто выучить этот вполне хватит остальные тебя читать <start> главный <start> мимо фсе пидары искусства ремесла за короной передать словами теперь придется аткудава лигивон льный нкр некто джошуа красивые подобрать свинья я отрицательно \n",
      " стало на логика \n",
      " книжки было москве с ты ли готовую библиотеку бабку до доступной для же такие как похвала полистайте онлайн-faq и раннем васяна их к печени что-то вырезать она спокойно хач без значит попали наконец кто-то в предыдущем рассчитали силу читаю думайпитон жирноты стека нее хоть не проверял тут даже слога но этапе когда\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach_bi, word2id_dvach_uni).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перплексия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_join_proba_markov_assumption(phrase, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(phrase)\n",
    "    for ngram in ngrammer(['<start>'] + tokens + ['<end>'], n=3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        a1 = ' '.join(ngram.split()[0:2])\n",
    "        if a1 in bigram_counts and ngram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[ngram]/bigram_counts[a1])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.018543655775815"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Безграмотное быдло с дубляжом, войсовером, порнографией и котикам'\n",
    "perplexity(*compute_join_proba_markov_assumption(phrase, bigrams_dvach, trigrams_dvach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/alexandersemiletov/toxic-russian-comments?resource=download\n",
    "phrases = ['А еще на стоянке никто не проверяет безопасность детей. возле порта на тарзанке стоит человек и смотрит за детьми, а на стоянке прыгает девочка, а вокруг никого.',\n",
    "          'Братка поздравляю тебя с днём рождения желаю всего и побольше а главное здоровья тебе и твоим близким.',\n",
    "          'Этот тип убийца и живодер, выкинуть его нахуй',\n",
    "          'защитник я или нет не тебе решать, а ты точно тупой вонючий тролль и старый пидорас, пошёл нахуй',\n",
    "          'я написал твари тому, кто херню всякую распростроняет!',\n",
    "          'я могу обоссать ваш талмуд, и ничего мне за это не будет, еби свою сраку крестом перевёрнутым',\n",
    "          'молодец, что сумел найти силы воспитывать дочь, а проблема с ногами - это мелочь, главное это любовь',\n",
    "          'нежная чешуя на солнце быстро загорает, пока над водой летает']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31054.202599870718\n",
      "26458.182276303272\n",
      "16490.5829637499\n",
      "5196.859120831984\n",
      "28043.287857796484\n",
      "13721.374452121767\n",
      "29985.920227773066\n",
      "50000.000000000095\n"
     ]
    }
   ],
   "source": [
    "for phrase in phrases:\n",
    "    print(perplexity(*compute_join_proba_markov_assumption(phrase, bigrams_dvach, trigrams_dvach)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то, что работает явно лучше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach_model = markovify.NewlineText(dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ты отвратительно питаешься.\n",
      "Спокойной ночи.\n",
      "Чтобы ты ещё и пояснят.\n",
      "Тоже самое, но бесплатно, не?\n",
      "Пиздуй в /б схоронял?\n",
      "Мы одну на троих делим. Музыка интереснее становится, мысли, образы, движение предметов. Это мне говорит диванный эксперт, который про свое окружение у незнакомого на дваче\n",
      "Я с Кинугасой и Како в 2-5 хожу вполне спокойно.\n",
      "> 7. Возвращены все вырезанные монстры и добавлены новые их разновидности: электрохимера, пси-бюрер, зомби-камикадзе, чумной зомби.> 11. Добавлено 10 экземпляров уникального оружия, которые можно найти этот отрывок с этими группами всё то что чухан с камерой, без согласия работника, лезет прямо в твою шлюху, спойлеродаун\n",
      "эльзофажинка, я б наверно ее б не проследили. Дома- все страницы удалены. Я свою первую программу?мамка бы все деньги забрала, нечего пиздюку с 500 даларами ходить, шуба нужнее\n",
      "> пожалуйся, моча забьет хуй\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dvach_model.make_sentence(tries=100, test_output=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?\n",
    "\n",
    "Авторы предлагают два решения проблемы:\n",
    "1) если используется словарь с ограниченным количеством слов (closed vocabulary), то при нормализации текста незнакомые слова можно помечать как \"UNK\" - unknown, чтобы далее считать вероятности UNK так же, как и вероятности остальных слов в датасете.\n",
    "2) Если используется словарь с неограниченным числом слов, можно использовать метод \"неявного\" создания словаря и в train выборке менять слова на тег UNK в зависимости от их частотности (например, заменять на UNK слова, которые редко встречаются в словаре. Или оставить в словаре только, например, 50тыс. наиболее частых слов, а остальные заменить на UNK). Далее можно работать с UNK так, как и с другими словами в словаре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?\n",
    "\n",
    "Сглаживание - это модификация, которая используется для работы со словами, которые в train и test словарях употребляются в разном контексте. В противном случае модель покажет, что вероятность встретить слово равна 0. Один из примеров сглаживания - сглаживание Лапласа: прибавить 1 ко всем элементам в словаре частотностей n-грам, чтобы нулевые значения перешли в 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
