{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orlovaea/nlp_ha/blob/main/nlp_ha10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание № 10. Машинный перевод"
      ],
      "metadata": {
        "id": "76Y_pqUL3m1S"
      },
      "id": "76Y_pqUL3m1S"
    },
    {
      "cell_type": "raw",
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами."
      ],
      "metadata": {
        "id": "a0086f61"
      },
      "id": "a0086f61"
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install unzip"
      ],
      "metadata": {
        "id": "e3b0ea3c",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:46:32.000080Z",
          "iopub.execute_input": "2023-04-13T08:46:32.000631Z",
          "iopub.status.idle": "2023-04-13T08:46:35.635423Z",
          "shell.execute_reply.started": "2023-04-13T08:46:32.000585Z",
          "shell.execute_reply": "2023-04-13T08:46:35.634141Z"
        },
        "trusted": true,
        "outputId": "2aaaa501-3431-4d76-94db-60450b47a07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nunzip is already the newest version (6.0-25ubuntu1.1).\n0 upgraded, 0 newly installed, 0 to remove and 76 not upgraded.\n",
          "output_type": "stream"
        }
      ],
      "id": "e3b0ea3c"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tokenizers matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d650e9eb",
        "outputId": "16aecd0e-9060-4289-f372-c1a2372d1953",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-04-13T08:46:35.638241Z",
          "iopub.execute_input": "2023-04-13T08:46:35.638584Z",
          "iopub.status.idle": "2023-04-13T08:46:45.424115Z",
          "shell.execute_reply.started": "2023-04-13T08:46:35.638538Z",
          "shell.execute_reply": "2023-04-13T08:46:45.422726Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.13.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.5.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (4.38.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (9.4.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (23.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.4.4)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.4.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "id": "d650e9eb"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "947b3313",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:46:45.427664Z",
          "iopub.execute_input": "2023-04-13T08:46:45.427998Z",
          "iopub.status.idle": "2023-04-13T08:46:45.493867Z",
          "shell.execute_reply.started": "2023-04-13T08:46:45.427963Z",
          "shell.execute_reply": "2023-04-13T08:46:45.492646Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "947b3313"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvUX7tdtc5pZ",
        "outputId": "3a1609b9-4478-442e-9437-3d7b65544c84",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:46:45.497305Z",
          "iopub.execute_input": "2023-04-13T08:46:45.498231Z",
          "iopub.status.idle": "2023-04-13T08:47:00.471593Z",
          "shell.execute_reply.started": "2023-04-13T08:46:45.498203Z",
          "shell.execute_reply": "2023-04-13T08:47:00.470403Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n--2023-04-13 08:46:46--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\nResolving data.statmt.org (data.statmt.org)... 129.215.197.184\nConnecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 121340806 (116M)\nSaving to: ‘opus.en-ru-train.ru.2’\n\nopus.en-ru-train.ru 100%[===================>] 115.72M  27.2MB/s    in 4.9s    \n\n2023-04-13 08:46:51 (23.4 MB/s) - ‘opus.en-ru-train.ru.2’ saved [121340806/121340806]\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n--2023-04-13 08:46:52--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\nResolving data.statmt.org (data.statmt.org)... 129.215.197.184\nConnecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 67760131 (65M)\nSaving to: ‘opus.en-ru-train.en.2’\n\nopus.en-ru-train.en 100%[===================>]  64.62M  21.0MB/s    in 3.1s    \n\n2023-04-13 08:46:56 (21.0 MB/s) - ‘opus.en-ru-train.en.2’ saved [67760131/67760131]\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n--2023-04-13 08:46:57--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\nResolving data.statmt.org (data.statmt.org)... 129.215.197.184\nConnecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 305669 (299K)\nSaving to: ‘opus.en-ru-test.ru.2’\n\nopus.en-ru-test.ru. 100%[===================>] 298.50K   692KB/s    in 0.4s    \n\n2023-04-13 08:46:58 (692 KB/s) - ‘opus.en-ru-test.ru.2’ saved [305669/305669]\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n--2023-04-13 08:46:59--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\nResolving data.statmt.org (data.statmt.org)... 129.215.197.184\nConnecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 173307 (169K)\nSaving to: ‘opus.en-ru-test.en.2’\n\nopus.en-ru-test.en. 100%[===================>] 169.25K   528KB/s    in 0.3s    \n\n2023-04-13 08:47:00 (528 KB/s) - ‘opus.en-ru-test.en.2’ saved [173307/173307]\n\n",
          "output_type": "stream"
        }
      ],
      "id": "kvUX7tdtc5pZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
        "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-train.ru', 'w')\n",
        "f.write(text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "38911d06",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:47:00.473288Z",
          "iopub.execute_input": "2023-04-13T08:47:00.473691Z",
          "iopub.status.idle": "2023-04-13T08:47:02.077480Z",
          "shell.execute_reply.started": "2023-04-13T08:47:00.473630Z",
          "shell.execute_reply": "2023-04-13T08:47:02.075903Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "38911d06"
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents = open('opus.en-ru-train.en').read().lower().splitlines()\n",
        "ru_sents = open('opus.en-ru-train.ru').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "e110ff04",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:47:02.080427Z",
          "iopub.execute_input": "2023-04-13T08:47:02.080863Z",
          "iopub.status.idle": "2023-04-13T08:47:06.412199Z",
          "shell.execute_reply.started": "2023-04-13T08:47:02.080818Z",
          "shell.execute_reply": "2023-04-13T08:47:06.411062Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "e110ff04"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(WordPiece(), )\n",
        "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_en = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en )\n",
        "\n",
        "tokenizer_ru = Tokenizer(WordPiece(), )\n",
        "tokenizer_ru.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_ru = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\", ])\n",
        "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru )"
      ],
      "metadata": {
        "id": "0be060a4",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:47:06.414067Z",
          "iopub.execute_input": "2023-04-13T08:47:06.414536Z",
          "iopub.status.idle": "2023-04-13T08:48:09.223159Z",
          "shell.execute_reply.started": "2023-04-13T08:47:06.414494Z",
          "shell.execute_reply": "2023-04-13T08:48:09.221841Z"
        },
        "trusted": true,
        "outputId": "f863ea6d-1a7a-4576-fc44-d4552d1ce204"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\n\n\n\n\n",
          "output_type": "stream"
        }
      ],
      "id": "0be060a4"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en.decoder = decoders.WordPiece()\n",
        "tokenizer_ru.decoder = decoders.WordPiece()"
      ],
      "metadata": {
        "id": "xWX3xnMUzbdt",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:48:09.224654Z",
          "iopub.execute_input": "2023-04-13T08:48:09.225692Z",
          "iopub.status.idle": "2023-04-13T08:48:09.231135Z",
          "shell.execute_reply.started": "2023-04-13T08:48:09.225650Z",
          "shell.execute_reply": "2023-04-13T08:48:09.229792Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "xWX3xnMUzbdt"
    },
    {
      "cell_type": "code",
      "source": [
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "tokenizer_en.save('tokenizer_en')\n",
        "tokenizer_ru.save('tokenizer_ru')"
      ],
      "metadata": {
        "id": "496d0ea7",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:48:09.233250Z",
          "iopub.execute_input": "2023-04-13T08:48:09.234266Z",
          "iopub.status.idle": "2023-04-13T08:48:09.256380Z",
          "shell.execute_reply.started": "2023-04-13T08:48:09.234229Z",
          "shell.execute_reply": "2023-04-13T08:48:09.255341Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "496d0ea7"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
      ],
      "metadata": {
        "id": "f4661964",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:48:09.261589Z",
          "iopub.execute_input": "2023-04-13T08:48:09.261955Z",
          "iopub.status.idle": "2023-04-13T08:48:09.323293Z",
          "shell.execute_reply.started": "2023-04-13T08:48:09.261916Z",
          "shell.execute_reply": "2023-04-13T08:48:09.322059Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "f4661964"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, target=False):\n",
        "    if target:\n",
        "        return [tokenizer.token_to_id('[START]')] + tokenizer.encode(text).ids + \\\n",
        "                [tokenizer.token_to_id('[END]')]\n",
        "    else:\n",
        "        return tokenizer.encode(text).ids "
      ],
      "metadata": {
        "id": "dc003758",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:48:09.326702Z",
          "iopub.execute_input": "2023-04-13T08:48:09.327370Z",
          "iopub.status.idle": "2023-04-13T08:48:09.333634Z",
          "shell.execute_reply.started": "2023-04-13T08:48:09.327329Z",
          "shell.execute_reply": "2023-04-13T08:48:09.332623Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "dc003758"
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, True) for t in ru_sents]"
      ],
      "metadata": {
        "id": "7fc2dae1",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:48:09.335085Z",
          "iopub.execute_input": "2023-04-13T08:48:09.336125Z",
          "iopub.status.idle": "2023-04-13T08:49:22.139252Z",
          "shell.execute_reply.started": "2023-04-13T08:48:09.336085Z",
          "shell.execute_reply": "2023-04-13T08:49:22.138052Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "7fc2dae1"
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en = np.max([len(x) for x in X_en])\n",
        "max_len_ru = np.max([len(x) for x in X_ru])"
      ],
      "metadata": {
        "id": "281b5b90",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:22.140957Z",
          "iopub.execute_input": "2023-04-13T08:49:22.142069Z",
          "iopub.status.idle": "2023-04-13T08:49:22.475974Z",
          "shell.execute_reply.started": "2023-04-13T08:49:22.142027Z",
          "shell.execute_reply": "2023-04-13T08:49:22.474880Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "281b5b90"
    },
    {
      "cell_type": "code",
      "source": [
        "#в seq2seq длины могут быть разными\n",
        "max_len_en, max_len_ru = 45, 48"
      ],
      "metadata": {
        "id": "5cc0a376",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:22.477533Z",
          "iopub.execute_input": "2023-04-13T08:49:22.478256Z",
          "iopub.status.idle": "2023-04-13T08:49:22.484390Z",
          "shell.execute_reply.started": "2023-04-13T08:49:22.478213Z",
          "shell.execute_reply": "2023-04-13T08:49:22.483201Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "5cc0a376"
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f4f31fa",
        "outputId": "85345521-b21a-4bc1-96e2-028ecd737034",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:22.485694Z",
          "iopub.execute_input": "2023-04-13T08:49:22.486650Z",
          "iopub.status.idle": "2023-04-13T08:49:22.499204Z",
          "shell.execute_reply.started": "2023-04-13T08:49:22.486614Z",
          "shell.execute_reply": "2023-04-13T08:49:22.497913Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 60,
          "output_type": "execute_result",
          "data": {
            "text/plain": "1"
          },
          "metadata": {}
        }
      ],
      "id": "3f4f31fa"
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              X_en, maxlen=max_len_en, padding='post', value=PAD_IDX)\n",
        "\n",
        "X_ru_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[1:] for x in X_ru], maxlen=max_len_ru-1, padding='post', \n",
        "              value=PAD_IDX)\n",
        "\n",
        "X_ru_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[:-1] for x in X_ru], maxlen=max_len_ru-1, \n",
        "              padding='post', value=PAD_IDX)"
      ],
      "metadata": {
        "id": "49ae735e",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:22.501871Z",
          "iopub.execute_input": "2023-04-13T08:49:22.502303Z",
          "iopub.status.idle": "2023-04-13T08:49:35.069852Z",
          "shell.execute_reply.started": "2023-04-13T08:49:22.502266Z",
          "shell.execute_reply": "2023-04-13T08:49:35.068601Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "49ae735e"
    },
    {
      "cell_type": "code",
      "source": [
        "(X_en_train, X_en_valid, \n",
        "X_ru_dec_train, X_ru_dec_valid, \n",
        "X_ru_out_train, X_ru_out_valid) = train_test_split(X_en, \n",
        "                                                  X_ru_dec, \n",
        "                                                  X_ru_out, \n",
        "                                                  test_size=0.05)"
      ],
      "metadata": {
        "id": "25fa5f05",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.073622Z",
          "iopub.execute_input": "2023-04-13T08:49:35.073931Z",
          "iopub.status.idle": "2023-04-13T08:49:35.702150Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.073902Z",
          "shell.execute_reply": "2023-04-13T08:49:35.701095Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "25fa5f05"
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    # Считаем скалярное произведение между запросом (query) и ключом (key), транспонируя ключ\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # Получаем глубину (размерность) ключа и преобразуем ее во float\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "\n",
        "    # Делим результат скалярного произведения на квадратный корень из глубины\n",
        "    # Это делается для уменьшения влияния больших значений и стабилизации градиентов во время обучения\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # Если есть маска, применяем ее к логитам, чтобы обнулить нежелательные значения\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    # Применяем функцию softmax для получения весов внимания\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    # Умножаем веса внимания на значения (value) для получения итогового результата\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "656be820",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.703735Z",
          "iopub.execute_input": "2023-04-13T08:49:35.704102Z",
          "iopub.status.idle": "2023-04-13T08:49:35.712117Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.704065Z",
          "shell.execute_reply": "2023-04-13T08:49:35.711121Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "656be820"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads  # количество голов для внимания\n",
        "        self.d_model = d_model  # размерность вектора модели\n",
        "\n",
        "        # Убеждаемся, что размерность модели делится нацело на количество голов\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads  # размерность каждой головы\n",
        "\n",
        "        # Создаем полносвязные слои для запроса, ключа и значения\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        # Создаем последний полносвязный слой\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        # Разделяем входные данные на головы\n",
        "        inputs = tf.reshape(\n",
        "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # Пропускаем запрос, ключ и значение через соответствующие полносвязные слои\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # Разделяем запрос, ключ и значение на головы \n",
        "        # то есть просто разрезаем вектора на num_heads частей \n",
        "        # и сравниваем все части между собой\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # Выполняем механизм внимания с масштабированным скалярным произведением\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # Объединяем головы вместе (склеиваем векторы в один)\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Пропускаем объединенное внимание через дополнительный полносвязный слой\n",
        "        # Он просто добавляет сложности нашей модели\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "f4b51870",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.713757Z",
          "iopub.execute_input": "2023-04-13T08:49:35.714398Z",
          "iopub.status.idle": "2023-04-13T08:49:35.732013Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.714362Z",
          "shell.execute_reply": "2023-04-13T08:49:35.731089Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "f4b51870"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "5c48cea2",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.733482Z",
          "iopub.execute_input": "2023-04-13T08:49:35.733922Z",
          "iopub.status.idle": "2023-04-13T08:49:35.750556Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.733885Z",
          "shell.execute_reply": "2023-04-13T08:49:35.749437Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "5c48cea2"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "21c0c899",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.751993Z",
          "iopub.execute_input": "2023-04-13T08:49:35.752476Z",
          "iopub.status.idle": "2023-04-13T08:49:35.761209Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.752441Z",
          "shell.execute_reply": "2023-04-13T08:49:35.760279Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "21c0c899"
    },
    {
      "cell_type": "code",
      "source": [
        "mask = 1 - tf.linalg.band_part(tf.ones((3, 3)), -1, 0)\n",
        "mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.762988Z",
          "iopub.execute_input": "2023-04-13T08:49:35.763376Z",
          "iopub.status.idle": "2023-04-13T08:49:35.777754Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.763341Z",
          "shell.execute_reply": "2023-04-13T08:49:35.776606Z"
        },
        "trusted": true,
        "id": "RBuhCBby3eDF",
        "outputId": "0f2d29ea-5f6b-48b3-fe8c-e5f8eec144c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 67,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 1., 1.],\n       [0., 0., 1.],\n       [0., 0., 0.]], dtype=float32)>"
          },
          "metadata": {}
        }
      ],
      "id": "RBuhCBby3eDF"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "e120bbe5",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.779780Z",
          "iopub.execute_input": "2023-04-13T08:49:35.780584Z",
          "iopub.status.idle": "2023-04-13T08:49:35.790676Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.780517Z",
          "shell.execute_reply": "2023-04-13T08:49:35.790006Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "e120bbe5"
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    #call_mha\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "68472627",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.793249Z",
          "iopub.execute_input": "2023-04-13T08:49:35.794171Z",
          "iopub.status.idle": "2023-04-13T08:49:35.809225Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.794144Z",
          "shell.execute_reply": "2023-04-13T08:49:35.808175Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "68472627"
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "89dcc42e",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.810662Z",
          "iopub.execute_input": "2023-04-13T08:49:35.811992Z",
          "iopub.status.idle": "2023-04-13T08:49:35.828651Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.811955Z",
          "shell.execute_reply": "2023-04-13T08:49:35.827590Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "89dcc42e"
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "    attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "a2f90e7c",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.832115Z",
          "iopub.execute_input": "2023-04-13T08:49:35.832387Z",
          "iopub.status.idle": "2023-04-13T08:49:35.845119Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.832362Z",
          "shell.execute_reply": "2023-04-13T08:49:35.844093Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "a2f90e7c"
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "8e9f89b8",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.847874Z",
          "iopub.execute_input": "2023-04-13T08:49:35.848751Z",
          "iopub.status.idle": "2023-04-13T08:49:35.859465Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.848710Z",
          "shell.execute_reply": "2023-04-13T08:49:35.858382Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "8e9f89b8"
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                max_len,\n",
        "                name=\"transformer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "    \n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "    \n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(\n",
        "                          vocab_size=vocab_size[0],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[0],\n",
        "                        )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(\n",
        "                          vocab_size=vocab_size[1],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[1],\n",
        "                        )(inputs=[dec_inputs, enc_outputs, \n",
        "                                  look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "e356741b",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.865880Z",
          "iopub.execute_input": "2023-04-13T08:49:35.866136Z",
          "iopub.status.idle": "2023-04-13T08:49:35.878505Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.866112Z",
          "shell.execute_reply": "2023-04-13T08:49:35.877434Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "e356741b"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none',)\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss = L(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "6c35ce0f",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.880243Z",
          "iopub.execute_input": "2023-04-13T08:49:35.880757Z",
          "iopub.status.idle": "2023-04-13T08:49:35.894569Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.880721Z",
          "shell.execute_reply": "2023-04-13T08:49:35.893415Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "6c35ce0f"
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# small model\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()),\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT,\n",
        "    max_len=[max_len_en, max_len_ru])\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model_ruen',\n",
        "                                            monitor='val_loss',\n",
        "                                            verbose=1,\n",
        "                                            save_weights_only=True,\n",
        "                                            save_best_only=True,\n",
        "                                            mode='min',\n",
        "                                            save_freq='epoch')"
      ],
      "metadata": {
        "id": "542fcc43",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:35.895961Z",
          "iopub.execute_input": "2023-04-13T08:49:35.896486Z",
          "iopub.status.idle": "2023-04-13T08:49:38.618632Z",
          "shell.execute_reply.started": "2023-04-13T08:49:35.896451Z",
          "shell.execute_reply": "2023-04-13T08:49:38.617598Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "542fcc43"
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit((X_en_train, X_ru_dec_train), X_ru_out_train, \n",
        "             validation_data=((X_en_valid, X_ru_dec_valid), X_ru_out_valid),\n",
        "             batch_size=500,\n",
        "             epochs=2,\n",
        "             callbacks=[checkpoint]\n",
        "             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "308a8e81",
        "outputId": "981be9b3-524f-4e0a-fc93-c44bddd4bf3c",
        "execution": {
          "iopub.status.busy": "2023-04-13T08:49:38.620144Z",
          "iopub.execute_input": "2023-04-13T08:49:38.620497Z",
          "iopub.status.idle": "2023-04-13T09:38:25.532511Z",
          "shell.execute_reply.started": "2023-04-13T08:49:38.620461Z",
          "shell.execute_reply": "2023-04-13T09:38:25.531532Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/2\n1900/1900 [==============================] - ETA: 0s - loss: 1.3727 - accuracy: 0.1089\nEpoch 1: val_loss improved from inf to 0.98438, saving model to model_ruen\n1900/1900 [==============================] - 1503s 783ms/step - loss: 1.3727 - accuracy: 0.1089 - val_loss: 0.9844 - val_accuracy: 0.1458\nEpoch 2/2\n1900/1900 [==============================] - ETA: 0s - loss: 0.9229 - accuracy: 0.1521\nEpoch 2: val_loss improved from 0.98438 to 0.85481, saving model to model_ruen\n1900/1900 [==============================] - 1424s 749ms/step - loss: 0.9229 - accuracy: 0.1521 - val_loss: 0.8548 - val_accuracy: 0.1614\n",
          "output_type": "stream"
        },
        {
          "execution_count": 76,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<keras.callbacks.History at 0x76a16f90dd50>"
          },
          "metadata": {}
        }
      ],
      "id": "308a8e81"
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "    input_ids = encode(text, tokenizer_en, target=False)\n",
        "\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [input_ids], maxlen=max_len_en, padding='post', \n",
        "                                       # важно не забыть паддинг с нужным id\n",
        "                                       value=PAD_IDX)\n",
        "\n",
        "    \n",
        "    \n",
        "    output_ids = [tokenizer_ru.token_to_id('[START]') ]\n",
        "    \n",
        "    pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False).numpy()\n",
        " \n",
        "    \n",
        "    while pred.argmax(2)[0][-1] not in [tokenizer_ru.token_to_id('[END]')]:\n",
        "        if len(output_ids) >= max_len_ru:\n",
        "            break\n",
        "        # можно занизить скор тэга UNK чтобы он никогда не генерировался\n",
        "        pred[:, :, tokenizer_ru.token_to_id('[UNK]')] = -100\n",
        "\n",
        "        output_ids.append(pred.argmax(2)[0][-1])\n",
        "        pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False).numpy()\n",
        "\n",
        "    return tokenizer_ru.decode(output_ids[1:], )\n"
      ],
      "metadata": {
        "id": "63762869",
        "execution": {
          "iopub.status.busy": "2023-04-13T09:38:25.533990Z",
          "iopub.execute_input": "2023-04-13T09:38:25.534462Z",
          "iopub.status.idle": "2023-04-13T09:38:25.543852Z",
          "shell.execute_reply.started": "2023-04-13T09:38:25.534424Z",
          "shell.execute_reply": "2023-04-13T09:38:25.542688Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "63762869"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-13T09:38:25.545381Z",
          "iopub.execute_input": "2023-04-13T09:38:25.546004Z",
          "iopub.status.idle": "2023-04-13T09:38:35.475079Z",
          "shell.execute_reply.started": "2023-04-13T09:38:25.545965Z",
          "shell.execute_reply": "2023-04-13T09:38:35.473160Z"
        },
        "trusted": true,
        "id": "hhOWlsGY3eDK",
        "outputId": "6c4a87f2-6fdf-4c7a-ac8f-dbb69559585d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "id": "hhOWlsGY3eDK"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-13T09:38:35.477059Z",
          "iopub.execute_input": "2023-04-13T09:38:35.477364Z",
          "iopub.status.idle": "2023-04-13T09:38:35.483317Z",
          "shell.execute_reply.started": "2023-04-13T09:38:35.477332Z",
          "shell.execute_reply": "2023-04-13T09:38:35.482294Z"
        },
        "trusted": true,
        "id": "dLucecYJ3eDK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dLucecYJ3eDK"
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents_test = open('opus.en-ru-test.en').read().lower().splitlines()\n",
        "ru_sents_test = open('opus.en-ru-test.ru').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "TbLvCxxlFCn6",
        "execution": {
          "iopub.status.busy": "2023-04-13T09:38:35.484954Z",
          "iopub.execute_input": "2023-04-13T09:38:35.485771Z",
          "iopub.status.idle": "2023-04-13T09:38:35.524351Z",
          "shell.execute_reply.started": "2023-04-13T09:38:35.485711Z",
          "shell.execute_reply": "2023-04-13T09:38:35.523440Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "TbLvCxxlFCn6"
    },
    {
      "cell_type": "code",
      "source": [
        "translations = []\n",
        "\n",
        "for i in range(len(ru_sents_test)):\n",
        "    translations.append(translate(en_sents_test[i]))"
      ],
      "metadata": {
        "id": "Dy0cRSoLFC0k",
        "execution": {
          "iopub.status.busy": "2023-04-13T09:38:35.525833Z",
          "iopub.execute_input": "2023-04-13T09:38:35.526184Z",
          "iopub.status.idle": "2023-04-13T10:28:49.037045Z",
          "shell.execute_reply.started": "2023-04-13T09:38:35.526150Z",
          "shell.execute_reply": "2023-04-13T10:28:49.035906Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "Dy0cRSoLFC0k"
    },
    {
      "cell_type": "code",
      "source": [
        "bleus = []\n",
        "\n",
        "for i, t in enumerate(translations):\n",
        "    reference = tokenizer_ru.encode(t).tokens\n",
        "    hypothesis = tokenizer_ru.encode(ru_sents_test[i]).tokens\n",
        "\n",
        "    bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  auto_reweigh=True))"
      ],
      "metadata": {
        "id": "hx3kUhHQFhMQ",
        "execution": {
          "iopub.status.busy": "2023-04-13T10:28:49.038377Z",
          "iopub.execute_input": "2023-04-13T10:28:49.038844Z",
          "iopub.status.idle": "2023-04-13T10:28:49.622007Z",
          "shell.execute_reply.started": "2023-04-13T10:28:49.038798Z",
          "shell.execute_reply": "2023-04-13T10:28:49.620953Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "hx3kUhHQFhMQ"
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(bleus)/len(bleus))*100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJDX-Z-3GKDE",
        "outputId": "ee22d497-8f53-4522-89c2-aeb5e329cbed",
        "execution": {
          "iopub.status.busy": "2023-04-13T10:28:49.623609Z",
          "iopub.execute_input": "2023-04-13T10:28:49.624010Z",
          "iopub.status.idle": "2023-04-13T10:28:49.631161Z",
          "shell.execute_reply.started": "2023-04-13T10:28:49.623971Z",
          "shell.execute_reply": "2023-04-13T10:28:49.630080Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 83,
          "output_type": "execute_result",
          "data": {
            "text/plain": "42.30084332140855"
          },
          "metadata": {}
        }
      ],
      "id": "rJDX-Z-3GKDE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кажется, что модель работает довольно плохо. Возможно это связано с тем, что в обучении было всего 2 эпохи (но иначе это было бы очень долго...)"
      ],
      "metadata": {
        "id": "ThRGHK0b3eDL"
      },
      "id": "ThRGHK0b3eDL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара. "
      ],
      "metadata": {
        "id": "fsoFbExp4FM2"
      },
      "id": "fsoFbExp4FM2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back translation - наиболее распространенная в машинном переводе техника для аугментации данных. Идея в том, что ограниченный по объему параллельный корпус можно \"обогатить\", используя моноязычный корпус для языка перевода (target) и создавая на его основе дополнительные битексты для обучения MT модели. Такая техника позволяет увеличить исходный набор данных для обучения модели и, соответственно, улучшить ее качество. \n",
        "\n",
        "Применение к паре en-ru (from English to Russian):\n",
        "- на основе параллельного корпуса обучаем модель, которая переводит в обратном порядке (с рус. на англ.)\n",
        "- используем полученную модель для перевода русского моноязычного корпуса на английский. Таким образом, у нас получается искусственный параллельный текст\n",
        "- добавляем полученный на предыдущем шаге искусственный битекст к исходным трейн данным и обучаем модель для перевода в нужном порядке (с англ. на рус.)\n",
        "Таким образом, модель обучается на большем объеме данных"
      ],
      "metadata": {
        "id": "i1ePVXvB4Gdb"
      },
      "id": "i1ePVXvB4Gdb"
    }
  ]
}